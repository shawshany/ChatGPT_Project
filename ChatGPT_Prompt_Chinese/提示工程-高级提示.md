

@[TOC](文章目录)

---

# 前言

随着ChatGPT的大火，提示工程在大模型中的重要性不言而喻，本文参考国外[Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)完成国内中文版本的《提示工程指南》，希望能够和大家一起交流，分享及发现提示工程的美妙之处。文章所有内容可以在[ChatGPT_Prompt_Chinese](https://github.com/shawshany/ChatGPT_Project/tree/main/ChatGPT_Prompt_Chinese)中找到。

到这个时候，应该很明显了，改进提示可以帮助在不同任务上获得更好的结果。这就是提示工程的整个理念。

虽然[那些例子很](https://xiaosheng.blog.csdn.net/article/details/129291872)有趣，但在我们深入探讨更高级的概念之前，让我们更正式地介绍一些概念。

---

# 一、Zero-Shot提示
今天的大语言模型在大量数据的训练和调整指令后，能够进行zero-shot任务执行。我们实际上在前面部分尝试了一些zero-shot示例。这里是我们使用的一个示例：

*Prompt:*
```
将文本分类为中性、负面或积极。

文本：我认为这个假期还可以。
情感：
```

*Output:*
```
中性
```
请注意，在上面的提示中，我们没有提供模型任何示例——这就是zero-shot功能的作用。当zero-shot无法正常工作时，建议在Prompt中提供演示或示例。下面我们将讨论称为 few-shot 提示的方法。

---

# 二、Few-Shot 提示
尽管大型语言模型已经展示了显著的zero-shot能力，但当使用zero-shot设置时，它们在更复杂的任务上仍然表现不足。为了改进这一点，few-shot prompt被用作一种技术，以实现上下文学习，在Prompt中提供演示，以引导模型获得更好的性能。这些演示作为后续示例的条件，我们希望模型生成响应。

让我们通过一个在[Brown et al. 2020](https://arxiv.org/abs/2005.14165)中提出的示例来演示 few-shot 提示。在这个例子中，任务是在一个句子中正确使用一个新词。

*Prompt:*
```
"Whatpu" 是一种生长在坦桑尼亚的小型毛茸茸的动物。使用这个词的一个例子是：
我们在非洲旅行时看到了这些非常可爱的 whatpus。

"Farduddle" 的意思是快速地上下跳动。使用这个词的一个例子是：
```

*Output:*
```
Farduddle 这个词的一个例子是：我弟弟在床上快速地上下跳动，就像他正在做 farduddle 一样。
```
我们可以清楚地观察到，模型通过仅提供一个示例（即1-shot）学会了如何执行任务。对于更困难的任务，我们可以尝试增加演示次数（例如3-shot、5-shot、10-shot等）。

根据 [Min et al. (2022)](https://arxiv.org/abs/2202.12837)，在进行 few-shot 时，以下是一些有关演示/范例的技巧：

* "演示中指定的标签空间和输入文本的分布都是关键因素（无论标签是否正确）"
* 即使只是使用随机标签，使用的格式也对性能起着关键作用，这比不使用标签要好得多。
* 附加结果表明，从真实标签分布中选择随机标签（而不是均匀分布）也有帮助。
让我们尝试一些示例。让我们首先尝试一个具有随机标签的示例（意味着将负面和积极标签随机分配给输入）：

*Prompt:*
```
这很棒！// 负面
这很糟糕！// 积极
哇，那部电影太棒了！// 积极
多么可怕的表演！//
```

*Output:*
```
这很棒！// 积极
这很糟糕！// 负面
哇，那部电影太棒了！// 积极
多么可怕的表演！// 负面
```
尽管标签已被随机化，我们仍然得到了正确的答案。请注意，我们也保留了格式，这也有所帮助。实际上，通过进一步实验，我们正在进行的新型 GPT 模型似乎变得更加强大，即使是在随机格式下也是如此。例如：

*Prompt:*
```
积极 这太棒了！
这很糟糕！负面
哇，那部电影太棒了！
积极
多么可怕的表演！--
```

*Output:*
```
负面
```
## Few-shot Prompting 的局限性
标准的 Few-shot Prompting 技术对于许多任务来说表现良好，但在处理更复杂的推理任务时仍不是一种完美的技术。让我们演示一下为什么会出现这种情况。您还记得我们之前提供的以下任务的例子吗？

```
这组数中的奇数加起来是一个偶数：15、32、5、13、82、7、1。

A:
```
如果我们再试一次，模型输出如下：
```
是的，这组数中的奇数加起来是一个偶数。

其中奇数有15、5、13、7、1，它们的和为41，是一个奇数。
```
这不是正确的回答，这不仅突显了这些系统的局限性，而且需要更先进的提示工程技术。让我们尝试添加一些示例，看看是否可以改善 Few-shot Prompting 的结果。
*Prompt:*
```
这组数中的奇数加起来是一个偶数：4、8、9、15、12、2、1。
A：答案是 False。

这组数中的奇数加起来是一个偶数：17、10、19、4、8、12、24。
A：答案是 True。

这组数中的奇数加起来是一个偶数：16、11、14、4、8、13、24。
A：答案是 True。

这组数中的奇数加起来是一个偶数：17、9、10、12、13、4、2。
A：答案是 False。

这组数中的奇数加起来是一个偶数：15、32、5、13、82、7、1。
A:
```

*Output:*
```
这组数中的奇数加起来是一个偶数：15、32、5、13、82、7、1。
A：答案是 False。
```
Few-shot Prompting 对于此类推理问题的已经能够获得可靠响应。上面的例子提供了任务的基本信息。如果您仔细观察，我们所介绍的任务类型涉及几个更多的推理步骤。换句话说，如果我们将问题分解成步骤，并向模型演示，可能会有所帮助。最近，[chain-of-thought (CoT) prompting](https://arxiv.org/abs/2201.11903)已经普及，用于解决更复杂的算术、常识和符号推理任务。

总的来说，提供示例对于解决一些任务是有用的。当零次提示和少次提示不足时，这可能意味着模型所学到的不足以胜任任务。因此，建议开始考虑微调您自己的模型或尝试更高级的提示技术。接下来，我们将讨论一种流行的提示技术，称为 chain-of-thought prompting，它已经广受欢迎。

---

# 三、Chain-of-Thought Prompting
[Wei等人（2022）](https://arxiv.org/abs/2201.11903)介绍了 chain-of-thought (CoT) prompting，它通过中间推理步骤实现复杂的推理能力。您可以将其与 Few-shot Prompting 结合使用，以在需要推理后响应的更复杂任务中获得更好的结果。

*Prompt:*
```
这组数中的奇数加起来是一个偶数：4、8、9、15、12、2、1。
A：将所有奇数相加（9、15、1），得到25。答案是 False。

这组数中的奇数加起来是一个偶数：17、10、19、4、8、12、24。
A：将所有奇数相加（17、19），得到36。答案是 True。

这组数中的奇数加起来是一个偶数：16、11、14、4、8、13、24。
A：将所有奇数相加（11、13），得到24。答案是 True。

这组数中的奇数加起来是一个偶数：17、9、10、12、13、4、2。
A：将所有奇数相加（17、9、13），得到39。答案是 False。

这组数中的奇数加起来是一个偶数：15、32、5、13、82、7、1。
A:
```

*Output:*
```
这组数中的奇数加起来是一个偶数：15、32、5、13、82、7、1。
A：答案是 False。
```
哇！我们可以看到当我们提供推理步骤时，结果是完美的。实际上，我们可以通过提供更少的示例来解决此任务，即只提供一个示例似乎就足够了：
*Prompt:*
```
这组数中的奇数加起来是一个偶数：4、8、9、15、12、2、1。
A：将所有奇数相加（9、15、1），得到25。答案是 False。

这组数中的奇数加起来是一个偶数：15、32、5、13、82、7、1。
A:
```

*Output:*
```
这组数中的奇数加起来是一个偶数：15、32、5、13、82、7、1。
A：答案是 False。因为将所有奇数相加（15、5、13、7、1），得到41，是一个奇数，不是偶数。
```

请记住，作者声称这是足够大的语言模型所产生的一种新兴能力。

---	

# 四、Zero-Shot CoT
最近提出的一个新思路是 [zero-shot CoT (Kojima等人 2022)](https://arxiv.org/abs/2205.11916)，其基本上涉及将“让我们一步步思考”添加到原始提示中。让我们尝试一个简单的问题，看看模型的表现如何：

*Prompt:*
```
我去市场买了10个苹果。我给了邻居2个苹果和给修理工2个苹果。然后我又去买了5个苹果，并吃了1个。我还剩下多少苹果？
```

*Output:*
```
我去市场买了10个苹果。我给了邻居2个苹果和给修理工2个苹果。然后我又去买了5个苹果，并吃了1个。我还剩下6个苹果。
```
答案是不正确的！现在让我们试试特殊提示的方式。
*Prompt:*
```
我去市场买了10个苹果。我给了邻居2个苹果和给修理工2个苹果。然后我又去买了5个苹果，并吃了1个。我还剩下多少苹果？

让我们一步步思考。
```

*Output:*
```
好的，让我们逐步解决这个问题。

我去市场买了10个苹果。

剩余苹果数：10个。

我给了邻居2个苹果和给修理工2个苹果。

剩余苹果数：6个。

然后我又去买了5个苹果。

剩余苹果数：11个。

并吃了1个。

剩余苹果数：10个。

所以，最后我还剩下10个苹果。
```

实际上，这个简单的提示在这个任务上非常有效，令人印象深刻。这在您没有太多示例可用于提示的情况下特别有用。

---

# 五、Self-Consistency
或许，对于提示工程来说，更高级的技术之一就是self-consistency。[Wang et al. (2022)](https://arxiv.org/pdf/2203.11171.pdf)提出了自一致性的概念，旨在“取代链式思维提示中使用的简单贪婪解码”。其思路是通过少次 CoT 来采样多条不同的推理路径，并使用生成结果来选择最一致的答案。这有助于提高 CoT 提示在涉及算术和常识推理任务中的性能。

让我们尝试下面这个算术推理的例子：

*Prompt:*
```
当我6岁时，我的妹妹的年龄是我的一半。现在我70岁了，我的妹妹现在多少岁？
```

*Output:*
```
根据题目所给的信息，当我6岁时，我的妹妹的年龄是3岁（即6的一半）。由此可知，我和我的妹妹之间的年龄差是3岁。

现在我70岁了，所以我的妹妹比我小3岁，也就是67岁。因此，我的妹妹现在是67岁。
```

不得不说，chatgpt在这块儿已经比davinci-3 “聪明”了不少，已经能够正确回答这类算数推理的问题了。

*Prompt:*

```
问：林地里有15棵树。种植工人今天将在这个区域种植树木。完成后，这里将有21棵树。种植工人今天种了多少棵树？
答：开始有15棵树。后来有21棵树。差异必须是他们种植的树木数量。因此，他们必须种植21-15=6棵树。答案是6。

问：停车场里有3辆汽车，又来了2辆汽车，现在停车场里有多少辆汽车？
答：停车场里已经有3辆汽车。又来了2辆，现在有3+2=5辆汽车。答案是5。

问：莉娅有32块巧克力，她的姐姐有42块。如果他们吃掉了35块，他们现在总共剩下多少块巧克力？
答：莉娅有32块巧克力，她的姐姐有42块，这意味着原来有32+42=74块巧克力。已经吃掉了35块。所以他们现在总共剩下74-35=39块巧克力。答案是39。

问：杰森有20个棒棒糖。他给了丹尼一些棒棒糖。现在杰森只剩下12个棒棒糖。杰森给了丹尼多少个棒棒糖？
答：杰森有20个棒棒糖。由于他现在只有12个，所以他必须把剩下的都给了丹尼。他给丹尼的棒棒糖数量必须是20-12=8个棒棒糖。答案是8。

问：肖恩有5个玩具。圣诞节时，他从他的父母那里各得到了两个玩具。现在他有多少个玩具？
答：他有5个玩具。他从妈妈那里得到了2个，所以他现在有5+2=7个玩具。然后他从爸爸那里得到了另外2个，所以他总共有7+2=9个玩具。答案是9。

Q：服务器房里原本有九台电脑。从周一到周四，每天又增加了五台电脑。现在服务器房里有多少台电脑？
A：从周一到周四一共有四天，每天增加五台电脑。所以总共增加了4 * 5 = 20 台电脑。一开始有九台电脑，现在总共有 9 + 20 = 29 台电脑。答案是29。

Q：Michael有58个高尔夫球。周二他丢了23个高尔夫球，周三又丢了2个。到周三结束时他还有多少高尔夫球？
A：Michael最初有58个高尔夫球。周二他丢了23个，所以他还剩下58 - 23 = 35个高尔夫球。周三又丢了2个，现在他还剩下35 - 2 = 33个高尔夫球。答案是33。

Q：Olivia有23美元。她买了五个3美元的百吉饼。她还剩下多少钱？
A：她花了5 * 3 = 15美元买了五个百吉饼。所以她还剩下23 - 15 = 8美元。

Q：当我6岁时，我的姐姐的年龄是我的一半。现在我70岁了，我的姐姐多少岁？
A：
```

*Output 1:*

```
当我6岁时，我的姐姐比我大3岁，因为我的姐姐的年龄是我的一半加上3岁。现在我70岁了，经过64年，我的年龄是当初的10倍。所以，如果我的姐姐当初比我大3岁，现在她的年龄是10 * 3 = 30岁。因此，我的姐姐现在应该是70 + 30 = 100岁。
```

*Output 2:*

```
当我6岁时，我的姐姐的年龄比我大6岁，也就是12岁（6岁的两倍）。现在我70岁了，过去的时间是70-6=64年。由于这64年中，我的姐姐年龄始终比我大6岁，所以她的年龄应该是70-64+6=12岁+6岁=18岁。因此，我的姐姐现在应该是18岁。
```

*Output 3:*
```
当我6岁时，我的姐姐比我年龄大3岁，因为3是6的一半。现在我70岁了，那么我的姐姐的年龄就是70-3=67岁。
```

计算最终答案涉及几个步骤（请查看论文了解详情），但为了简单起见，我们可以清楚地看到已经出现了大多数的答案，因此那将成为最终答案。

---

# 六、Generated Knowledge 提示

大语言模型（LLMs）不断得到改进，其中一种流行的技术是能够整合知识或信息，以帮助模型进行更准确的预测。

使用类似的想法，模型是否也可以在进行预测之前生成知识？这正是 [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf)在论文中尝试的内容——生成用作提示的知识。特别是，这对于常识推理等任务有多大帮助？

让我们试试一个简单的提示

*Prompt:*
```
高尔夫的一部分是试图获得比其他人更高的得分总数。是或不是？
```

*Output:*
```
是的，高尔夫的一部分是试图获得比其他人更高的得分总数。
```
这种错误揭示了大型语言模型在执行需要更多关于世界知识的任务时的局限性。我们如何通过知识生成来改进这一点？

首先，我们生成一些“知识”：
*Prompt:*

```
输入：希腊比墨西哥大。
知识：希腊面积约为131,957平方公里，而墨西哥约为1,964,375平方公里，使得墨西哥比希腊大了1,389%。

输入：眼镜总是会起雾。
知识：当汗水、呼吸和环境湿度中的水蒸气着陆在一个冷表面上时，会在眼镜镜片上产生冷凝，然后变成微小的液滴，形成你看到的雾气。与你的呼吸相比，镜片相对较冷，特别是在外部空气寒冷时。

输入：鱼有思维能力。
知识：鱼比它们表现出来的更聪明。在许多领域，例如记忆力，它们的认知能力与甚至超过非人灵长类动物等“高级”脊椎动物的认知能力。鱼的长期记忆帮助它们跟踪复杂的社交关系。

输入：一个人一生中吸烟量大，患肺癌的几率比正常人高。
知识：那些终身平均每天少于一支烟的人，患肺癌的风险是从未吸烟者的九倍。在每天吸烟1到10支烟之间的人中，死于肺癌的风险几乎比从未吸烟者高出12倍。

输入：一块石头和一颗卵石大小相同。
知识：卵石是岩石的一种，根据沉积学中的Udden-Wentworth比例尺，其粒径为4到64毫米。卵石通常被认为比颗粒（直径2到4毫米）大，比卵石（直径64到256毫米）小。

输入：高尔夫球的一部分是试图比其他人获得更高的分数。
知识：
```

*Knowledge 1:*
```
高尔夫球的目标是在球场上使用尽可能少的杆数将球打入洞中。高尔夫球手通过在每个洞上打出最少的杆数来获得更低的总分数。因此，高尔夫球的一部分是试图比其他人获得更低的分数，而不是更高的分数。
```

*Knowledge 2:*
```
知识：高尔夫球是一项球类运动，目的是将球打进被称为“洞”的小孔中，尽可能少地使用球杆。每个球场都有不同的“标准杆数”，即完成球场上所有洞的理论最少杆数。高尔夫球比赛通常是个人赛或团体赛，其中一部分是竞争比赛，试图比其他人获得更低的杆数，另一部分是计分赛，试图在规定杆数内取得最高的得分。
```
我们正在使用[Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf)的论文中提供的提示。

下一步是将知识整合并得出预测。我将问题重新格式化为问答格式，以指导答案的格式。
*Prompt:*
```
问题：高尔夫球的一部分是试图比其他人获得更高的分数。是或否？

知识：高尔夫球的目标是以最少的杆数打完一组洞。一般一场高尔夫球赛包括18个洞。每个洞在标准高尔夫球场上打一次。每一杆都算一分，总杆数用于确定比赛的获胜者。

解释和答案：
```

*Answer 1 :*
```
否。高尔夫球的目标是以最少的杆数打完一组洞，每一杆都算一分，总杆数用于确定比赛的获胜者。因此，高尔夫球的一部分是试图比其他人获得更低的分数。
```

```
问题：高尔夫球的一部分是试图比其他人获得更高的分数。是或否？

知识：高尔夫球是一项需要精密球杆技术的运动，参赛选手（或高尔夫球手）使用多种类型的球杆将球打进球场上的一系列洞中，以最少的杆数完成比赛。目标是用最少的杆数完成比赛，得分是通过计算每个洞上的杆数总和得出的。得分最低的球员获胜。

解释和答案：
```

*Answer 2 (confidence is a lot lower):*
```
回答：否。高尔夫球的目标是用最少的杆数完成比赛，得分是通过计算每个洞上的杆数总和得出的。因此，高尔夫球的一部分是试图比其他人获得更低的分数，得分最低的球员获胜。

```

在这个例子中发生了一些非常有趣的事情。在第一个答案中，模型非常自信，但在第二个答案中则不然。我为了演示目的而简化了这个过程，但在得出最终答案时还有一些细节需要考虑。请查阅论文以获取更多信息。

---

# 七、Automatic Prompt Engineer (APE)
![在这里插入图片描述](https://img-blog.csdnimg.cn/25c9c7a5b84d4f009da0d519749057c2.png)
[Zhou et al., (2022)](https://arxiv.org/abs/2211.01910)提出了一个名为“自动提示工程师”（APE）的框架，用于自动生成和选择指令。指令生成问题被构建为自然语言合成，使用LLMs解决黑盒优化问题来生成和搜索候选解决方案。

第一步涉及一个大型语言模型（作为推理模型），该模型给出输出演示，为任务生成指令候选项。这些候选解决方案将指导搜索过程。指令使用目标模型执行，然后根据计算的评估分数选择最适合的指令。

APE发现了比人工设计的“让我们一步一步地思考”（Kojima等人，2022年）更好的零样本CoT提示。

该提示“让我们一步一步地解决它，以确保我们有正确的答案。”引发了连锁思维，并提高了在MultiArith和GSM8K基准测试中的性能。

![在这里插入图片描述](https://img-blog.csdnimg.cn/4fd4be2507c2478c81573941137155b2.png)
本文涉及一个与提示工程相关的重要主题，即自动优化提示的想法。虽然本指南不会深入探讨这个主题，但如果您对此感兴趣，以下是几篇关键论文：
- [AutoPrompt](https://arxiv.org/abs/2010.15980) - 提出了一种基于梯度引导搜索的方法，用于自动创建各种任务的提示。
- [Prefix Tuning](https://arxiv.org/abs/2101.00190) - 一种轻量级的Fine-tuning替代方案，为NLG任务准备一个可训练的连续前缀。
- [Prompt Tuning](https://arxiv.org/abs/2104.08691) - 提出了一种通过反向传播学习软提示的机制。



---

# 总结
这篇文章我们介绍Prompt提示工程的一些高级示例，也对如何进一步调教大模型有了更深入的了解，在[带你玩转ChatGPT
](https://blog.csdn.net/u010665216/category_12217154.html)专栏中，我们会继续介绍ChatGPT的更多应用及实践案例。